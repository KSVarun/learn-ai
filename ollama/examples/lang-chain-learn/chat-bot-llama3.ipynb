{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend before going to Mars?\n",
      "\n",
      "Because he needed space!"
     ]
    }
   ],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "model=\"llama3\"\n",
    "llm = ChatOllama(model=model)\n",
    "# prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/concepts/#langchain-expression-language-lcel\n",
    "# chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# topic = {\"topic\": \"Space travel\"}\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "# print(chain.invoke({\"topic\": \"Space travel\"}))\n",
    "# async for chunks in chain.astream(topic):\n",
    "#     print(chunks, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 4049fe39-4bd7-4b45-a6fa-14c7b90d6f96 not found for run 5f158bc3-2671-444a-85d0-afd27ed3c4f6. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eck"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "model=\"llama3\"\n",
    "llm = ChatOllama(model=model)\n",
    "store = {}\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=256,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the prompt can take multiple variables\n",
    "# in the below example language is another variable\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "#         ),\n",
    "#         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a fast and helpful word predictor, given a sentence with the last word partially completed you should give rest of the letters of the word that would grammatically complete the word and no other characters\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#first should be prompt and then the model, if this is interchanged then it will fail langchain version \n",
    "# langchain                 0.2.5              pyhd8ed1ab_1    conda-forge\n",
    "# langchain-community       0.2.5                    pypi_0    pypi\n",
    "# langchain-core            0.2.9              pyhd8ed1ab_0    conda-forge\n",
    "# langchain-text-splitters  0.2.1              pyhd8ed1ab_0    conda-forge\n",
    "\n",
    "chain = ( RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) | prompt | llm )\n",
    "\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# with_message_history = RunnableWithMessageHistory(llm, get_session_history)\n",
    "\n",
    "# since we have multiple variables we need to specify where to get the messages and hence input_messages_key is passed for RunnableWithMessageHistory\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"new\"}}\n",
    "\n",
    "# to invoke when multiple variables are present\n",
    "\n",
    "# response = with_message_history.invoke(\n",
    "#     {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
    "#     config=config,\n",
    "# )\n",
    "\n",
    "# without streaming\n",
    "# response = with_message_history.invoke(\n",
    "#     {\"messages\":[HumanMessage(content=\"Hi! I'm Bob\")]},\n",
    "#     config=config,\n",
    "# )\n",
    "\n",
    "# response.content\n",
    "\n",
    "for r in with_message_history.stream(\n",
    "    {\"messages\":[HumanMessage(content=\"this is a test to ch\")]},\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run eb47eefe-6496-4b2b-8457-dc97460929ef not found for run 967ec865-226d-47f7-920f-7792b474e248. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You told me earlier, Bob! Your name is Bob!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What's my name?\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 878c72a0-f381-4d94-9b5d-150dadd62fff not found for run 2e8b8e75-86d6-4a88-8f5b-207868c581a1. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm glad I was able to get it right the first time. If you need any more help or just want to chat, I'm here for you, Bob!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Yes you are correct\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 313ed30a-8fee-4c3f-ba50-a37e87a08ad3 not found for run c887982c-51d8-4f2f-82ca-a2e63a76b169. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great, Bob! It sounds like you're quite the active person with a passion for swimming. And wow, writing code too? You must be pretty sharp!\\n\\nWhat kind of coding do you enjoy doing? Are you more into web development, game development, or maybe something else entirely?\\n\\nAnd by the way, how's your swimming going? Do you have a favorite spot to swim or any upcoming competitions/plans?\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = {\"configurable\": {\"session_id\": \"new\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"my age is 25 and i like to swim and write code\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 3d528c7b-9d66-4989-92ff-7c91ef44185e not found for run 5a0ac372-4314-4f50-8917-2dc5bd6b6352. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Whoa, that's awesome, Bob! Being a full-stack engineer means you're proficient in both front-end and back-end development. That's impressive!\\n\\nSo, what kind of projects have you worked on recently? Are you more interested in building scalable APIs or crafting user-friendly interfaces?\\n\\nAnd, since you mentioned swimming earlier, do you find that it helps you clear your mind and relax after a long day of coding?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"i am a full stack engineer\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 436205a7-1b4e-4a85-a375-e3874c20ccae not found for run 5708d663-47aa-4102-950d-c70494aa7d95. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I remember! Your name is Bob, right?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"what is my name\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollam-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
